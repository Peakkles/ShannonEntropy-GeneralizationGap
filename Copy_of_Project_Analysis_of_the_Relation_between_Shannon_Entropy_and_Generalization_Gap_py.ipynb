{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H0tS9vbVz-l",
        "outputId": "9059f2e2-fb2b-4b97-8898-ea13a4999feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================  ARGUMENTS  ================\n",
            "Epochs: 1\n",
            "Momentum: 0.9\n",
            "Learning rate: 0.1\n",
            "Width: 8\n",
            "Weight Decay: 0.0001\n",
            "Batch size: 128\n",
            "Skip Connections: True\n",
            "GPU: 0\n",
            "model: mlp\n",
            "hidden_nodes: 256\n",
            "dataset: CIFAR10\n",
            "=============================================\n",
            "['1_0.9_8_0.0001_0.1_128_True_0_232_mlp_256_CIFAR10_']\n",
            "1_0.9_8_0.0001_0.1_128_True_0_232_mlp_256_CIFAR10_.csv\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:17<00:00, 17.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2.236670703048706, 87.256, 2.087304931640625, 81.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.distributed as dist\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as udata\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "# write a function to return train and test laoders\n",
        "# Note that dataset argument can also be MNIST. So\n",
        "# You might want to use if else statements to handle\n",
        "# the cases for getting the dataloader.\n",
        "def get_dataloaders(dataset = 'CIFAR10', bs = 128, model = 'resnet'):\n",
        "\n",
        "    if dataset == 'CIFAR10':\n",
        "        if model == 'resnet':\n",
        "            transform_train = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                # transforms.RandomHorizontalFlip(0.5),\n",
        "                # transforms.RandomCrop(32, 2),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                ])\n",
        "            transform_test = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "                ])\n",
        "        else:\n",
        "            transform_train = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                # transforms.RandomHorizontalFlip(0.5),\n",
        "                # transforms.RandomCrop(32, 2),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "                transforms.Grayscale()\n",
        "                ])\n",
        "            transform_test = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "                transforms.Grayscale()\n",
        "                ])\n",
        "        trainset = torchvision.datasets.CIFAR10('./data', download=True, train=True, transform=transform_train)\n",
        "        testset = torchvision.datasets.CIFAR10('./data', download=True, train=False, transform=transform_test)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, bs, True)\n",
        "        testloader = torch.utils.data.DataLoader(testset, 1000, False)\n",
        "    else:\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,),(0.5,)),\n",
        "            transforms.Pad(2)\n",
        "            ])\n",
        "\n",
        "        trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
        "        testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, bs, True)\n",
        "        testloader = torch.utils.data.DataLoader(testset, 1000, False)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "class FlexibleMLP(nn.Module):\n",
        "    def __init__(self, hidden_size, hidden_layers):\n",
        "        super(FlexibleMLP, self).__init__()\n",
        "        layers = [nn.Linear(32*32, hidden_size), nn.ReLU()]\n",
        "        for _ in range(hidden_layers-1):\n",
        "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_size, 10))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        # complete the forward function\n",
        "        x = x.view(-1, 32*32)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, args, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.skip = args.skip\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        if self.skip:\n",
        "            self.shortcut = nn.Sequential()\n",
        "            if stride != 1 or in_planes != self.expansion*planes:\n",
        "                self.shortcut = nn.Sequential(\n",
        "                    nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride),\n",
        "                    nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.skip:\n",
        "            out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, args, num_classes=10, dataset = \"CIFAR10\"):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.args = args\n",
        "        print(self.args.skip)\n",
        "        self.in_planes = self.args.width * 8\n",
        "\n",
        "        if dataset == \"CIFAR10\":\n",
        "          input = 3\n",
        "        else:\n",
        "          input = 1\n",
        "        self.conv1 = nn.Conv2d(input, self.args.width * 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(self.args.width * 8)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, self.args.width * 8, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, self.args.width * 16, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, self.args.width * 32, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, self.args.width * 64, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(self.args.width * 64 * block.expansion, num_classes)\n",
        "        self.normed = False\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.args, self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet18_narrow(args, **kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], args, **kwargs, dataset = args.dataset)\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    ''' Function carry out training over the given dataloader'''\n",
        "\n",
        "    all_correct = 0\n",
        "    all_loss = 0\n",
        "    total_size = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        l = len(target)\n",
        "        total_size += l\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        #grad_list = [param.grad for param in list(model.parameters())]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #grad_list = [param.grad for param in list(model.parameters())]\n",
        "        output = model(data)\n",
        "        #loss = criterion(output, target)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        all_loss += loss.item() * l\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        correct = (predicted.to(device) == target).sum().item()\n",
        "        all_correct += correct\n",
        "\n",
        "    all_correct = all_correct / total_size\n",
        "    all_loss = all_loss / total_size\n",
        "    all_error = (1 - all_correct) * 100\n",
        "\n",
        "    return model, all_loss, all_error\n",
        "\n",
        "\n",
        "def test_epoch(model, test_loader, device):\n",
        "    ''' Function carry out test over the given dataloader'''\n",
        "    all_correct = 0\n",
        "    all_loss = 0\n",
        "    total_size = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            l = len(target)\n",
        "            total_size += l\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target) * l\n",
        "            all_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct = (predicted.to(device) == target).sum().item()\n",
        "            all_correct += correct\n",
        "\n",
        "    all_correct = all_correct / total_size\n",
        "    all_loss = all_loss / total_size\n",
        "    all_error = (1 - all_correct) * 100\n",
        "\n",
        "    return all_loss, all_error\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f')\n",
        "    # Add more arguments if needed\n",
        "    # you can use this tutorial as reference: https://www.youtube.com/watch?v=OxpBMNalsDM\n",
        "\n",
        "    parser.add_argument('--epochs', default=1, type=int)\n",
        "    parser.add_argument('--momentum', default=0.9, type=float)\n",
        "    parser.add_argument('--width', default=8, type=int)\n",
        "    parser.add_argument('--wd', default=1e-4, type=float)\n",
        "    parser.add_argument('--lr', default=0.1, type=float)\n",
        "    parser.add_argument('--batch_size', default=128, type=int)\n",
        "    parser.add_argument('--skip', default=True, action='store_false')\n",
        "    parser.add_argument('--gpu', default=0, type=int)\n",
        "    parser.add_argument('--seed', default=232, type=int)\n",
        "    parser.add_argument('--model', default=\"mlp\", type=str)\n",
        "    parser.add_argument('--hidden_nodes', default=256, type=int)\n",
        "    parser.add_argument('--dataset', default=\"CIFAR10\", type=str)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # parser.parse_args(['--sum', '7', '-1', '42'])\n",
        "\n",
        "    print(\"================  ARGUMENTS  ================\")\n",
        "    print(\"Epochs:\", args.epochs)\n",
        "    print(\"Momentum:\", args.momentum)\n",
        "    print(\"Learning rate:\", args.lr)\n",
        "    print(\"Width:\", args.width)\n",
        "    print(\"Weight Decay:\", args.wd)\n",
        "    print(\"Batch size:\", args.batch_size)\n",
        "    print(\"Skip Connections:\", args.skip)\n",
        "    print(\"GPU:\", args.gpu)\n",
        "    print(\"model:\", args.model)\n",
        "    print(\"hidden_nodes:\", args.hidden_nodes)\n",
        "    print(\"dataset:\", args.dataset)\n",
        "    print(\"=============================================\")\n",
        "\n",
        "    # TODO:\n",
        "    # EXP_DIR SHOULD DEPEND ONLY ON THE MODEL TYPE AND DATASET NAME\n",
        "    # MODIFY THIS ACCORDINGLY\n",
        "    EXP_DIR = args.dataset + \" \" + args.model\n",
        "    if EXP_DIR not in os.listdir():\n",
        "        os.mkdir(EXP_DIR)\n",
        "\n",
        "    # TODO:\n",
        "    # folder_name SHOULD DEPEND ON ALL THE HYPERPARAMETERS THAT ARE VARIED\n",
        "    # MODIFY THIS PART ACCORDINGLY\n",
        "    folder_name = '%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_'%(str(args.epochs),\n",
        "                                                          str(args.momentum),\n",
        "                                                          str(args.width),\n",
        "                                                          str(args.wd),\n",
        "                                                          str(args.lr),\n",
        "                                                          str(args.batch_size),\n",
        "                                                          str(args.skip),\n",
        "                                                          str(args.gpu),\n",
        "                                                          str(args.seed),\n",
        "                                                          str(args.model),\n",
        "                                                          str(args.hidden_nodes),\n",
        "                                                          str(args.dataset))\n",
        "    folder_dir = os.path.join(EXP_DIR, folder_name)\n",
        "\n",
        "\n",
        "    print(os.listdir(EXP_DIR))\n",
        "    if folder_name not in os.listdir(EXP_DIR):\n",
        "        os.mkdir(folder_dir)\n",
        "    doc_name = folder_name + '.csv'\n",
        "    print(doc_name)\n",
        "    csv_dir = os.path.join(folder_dir, doc_name)\n",
        "    params_dir = os.path.join(folder_dir, \"modelparams.pt\")\n",
        "\n",
        "    train_loader, test_loader = get_dataloaders(dataset = args.dataset, bs = args.batch_size, model = args.model)\n",
        "    gpu_name = 'cuda:%d'%args.gpu\n",
        "    device = gpu_name if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    if args.model == 'resnet':\n",
        "      model = resnet18_narrow(args).to(device)\n",
        "    else:\n",
        "      model = FlexibleMLP(args.hidden_nodes, args.width).to(device)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction = 'sum').to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr = args.lr,\n",
        "                            weight_decay = args.wd, momentum = args.momentum)\n",
        "\n",
        "    log_ls = []\n",
        "    # For loop for epoch\n",
        "    for e in tqdm(range(1,args.epochs+1)):\n",
        "\n",
        "        if e == 100: # learning rate drop by 10 fold after 100 epochs\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = g['lr']*0.1\n",
        "        elif e == 150: # another lr drop by 10 fold\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = g['lr']*0.1\n",
        "\n",
        "        # training\n",
        "        model, train_loss, train_error = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # testing\n",
        "        test_loss, test_error = test_epoch(model, test_loader, device)\n",
        "\n",
        "        # keeping track of the errors and losses\n",
        "        row_ls = [e, train_loss, train_error, test_loss, test_error]\n",
        "        print(row_ls)\n",
        "        log_ls.append(row_ls)\n",
        "\n",
        "        # updating and writing the csv\n",
        "        column_names = ['Epoch', 'Train Loss', 'Train Error', 'Test Loss', 'Test Error']\n",
        "        data_pd = pd.DataFrame(data = log_ls, columns = column_names)\n",
        "        data_pd.to_csv(csv_dir)\n",
        "\n",
        "    torch.save(model.state_dict(), params_dir) # saving model parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -9 python"
      ],
      "metadata": {
        "id": "OcAyc3QaaIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis"
      ],
      "metadata": {
        "id": "xeUaGsTSDuvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "SOFaxk4J2P8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import torchvision\n",
        "import torch.distributed as dist\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as udata\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def get_dataloaders(dataset = 'CIFAR10', bs = 128, model = 'resnet'):\n",
        "\n",
        "    if dataset == 'CIFAR10':\n",
        "        if model == 'resnet':\n",
        "            transform_train = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                # transforms.RandomHorizontalFlip(0.5),\n",
        "                # transforms.RandomCrop(32, 2),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                ])\n",
        "            transform_test = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "                ])\n",
        "        else:\n",
        "            transform_train = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                # transforms.RandomHorizontalFlip(0.5),\n",
        "                # transforms.RandomCrop(32, 2),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "                transforms.Grayscale()\n",
        "                ])\n",
        "            transform_test = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "                transforms.Grayscale()\n",
        "                ])\n",
        "        trainset = torchvision.datasets.CIFAR10('./data', download=True, train=True, transform=transform_train)\n",
        "        testset = torchvision.datasets.CIFAR10('./data', download=True, train=False, transform=transform_test)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, bs, True)\n",
        "        testloader = torch.utils.data.DataLoader(testset, 1000, False)\n",
        "    else:\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,),(0.5,)),\n",
        "            transforms.Pad(2)\n",
        "            ])\n",
        "\n",
        "        trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
        "        testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, bs, True)\n",
        "        testloader = torch.utils.data.DataLoader(testset, 1000, False)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "class FlexibleMLP(nn.Module):\n",
        "    def __init__(self, hidden_size, hidden_layers):\n",
        "        super(FlexibleMLP, self).__init__()\n",
        "        layers = [nn.Linear(32*32, hidden_size), nn.ReLU()]\n",
        "        for _ in range(hidden_layers-1):\n",
        "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_size, 10))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        # complete the forward function\n",
        "        x = x.view(-1, 32*32)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, args, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.skip = args.skip\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        if self.skip:\n",
        "            self.shortcut = nn.Sequential()\n",
        "            if stride != 1 or in_planes != self.expansion*planes:\n",
        "                self.shortcut = nn.Sequential(\n",
        "                    nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride),\n",
        "                    nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.skip:\n",
        "            out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, args, num_classes=10, dataset = \"CIFAR10\"):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.args = args\n",
        "        print(self.args.skip)\n",
        "        self.in_planes = self.args.width * 8\n",
        "\n",
        "        if dataset == \"CIFAR10\":\n",
        "          input = 3\n",
        "        else:\n",
        "          input = 1\n",
        "        self.conv1 = nn.Conv2d(input, self.args.width * 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(self.args.width * 8)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, self.args.width * 8, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, self.args.width * 16, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, self.args.width * 32, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, self.args.width * 64, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(self.args.width * 64 * block.expansion, num_classes)\n",
        "        self.normed = False\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.args, self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet18_narrow(args, **kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], args, **kwargs, dataset = args.dataset)\n",
        "\n",
        "\n",
        "def calc_shannon_entropy(model, dataloader, device):\n",
        "    '''\n",
        "    Write a function to calculate the average value of the shannon entropy\n",
        "    calculated over the given dataloader (ideally this is trainloader)\n",
        "    '''\n",
        "    shan_ent = 0\n",
        "    total = 0\n",
        "    m = torch.nn.Softmax()\n",
        "    for (data, target) in dataloader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      # print(output.shape)\n",
        "      softmaxed = m(output)\n",
        "      shan_ent -= torch.sum(softmaxed*torch.log2(softmaxed))\n",
        "      total += len(softmaxed)\n",
        "    shan_ent = shan_ent/total\n",
        "\n",
        "    return shan_ent\n",
        "\n",
        "def get_model_eval_comprehensive(line_worker, all_train_loader, all_test_loader, device):\n",
        "\n",
        "    line_worker.eval()\n",
        "    train_loss, train_correct = 0, 0\n",
        "    train_total_size = 0\n",
        "    for batch_idx, (data, target) in enumerate(all_train_loader):\n",
        "        train_total_size += len(target)\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        target = target.to(device, non_blocking=True)\n",
        "        output = line_worker(data)\n",
        "        train_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        train_correct += (predicted.to(device) == target).sum().item()\n",
        "    train_loss = train_loss/train_total_size\n",
        "    train_error = (1 - train_correct/train_total_size)* 100\n",
        "\n",
        "\n",
        "    line_worker.eval()\n",
        "    test_loss, test_correct = 0, 0\n",
        "    test_total_size = 0\n",
        "    for batch_idx, (data, target) in enumerate(all_test_loader):\n",
        "        test_total_size += len(target)\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        target = target.to(device, non_blocking=True)\n",
        "        output = line_worker(data)\n",
        "        test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        test_correct += (predicted.to(device) == target).sum().item()\n",
        "    test_loss = test_loss/test_total_size\n",
        "    test_error = (1 - test_correct/test_total_size)* 100\n",
        "\n",
        "    return train_loss, train_error, test_loss, test_error\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "\n",
        "# MAIN PROGRAM =================\n",
        "\n",
        "parser.add_argument('--model', default='mlp', type=str)\n",
        "parser.add_argument('--dataset', default=\"CIFAR10\", type=str)\n",
        "parser.add_argument('--gpu', default=0, type=int)\n",
        "args = parser.parse_args()\n",
        "logging.basicConfig(filename = 'file_aug.log',\n",
        "                    level = logging.DEBUG,\n",
        "                    format = '%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
        "\n",
        "exp_dir = './' + args.dataset + \"_\" + args.model #TODO\n",
        "device = 'cuda:0'\n",
        "model_name = args.model #TODO: Enter correct model name\n",
        "\n",
        "dirs = os.listdir(exp_dir)\n",
        "# dirs = [dr for dr in dirs if model_name in dr]\n",
        "# all_train_loader, all_test_loader = get_dataloaders()\n",
        "valley_csv = []\n",
        "\n",
        "for ind, dir in tqdm(enumerate(dirs)):\n",
        "\n",
        "    pull_dir = os.path.join(exp_dir, dir)\n",
        "    print(dir.split('_'))\n",
        "    args.momentum = float(dir.split('_')[1])\n",
        "    args.width = int(dir.split('_')[2])\n",
        "    args.wd = float(dir.split('_')[3])\n",
        "    args.lr = float(dir.split('_')[4])\n",
        "    args.batch_size = int(dir.split('_')[5])\n",
        "    args.model = dir.split('_')[9]\n",
        "    args.hidden_nodes = int(dir.split('_')[10])\n",
        "    args.dataset = dir.split('_')[11]\n",
        "\n",
        "    # TODO: Rewrite this parsing part depending on your convention\n",
        "    if dir.split('_')[6] == 'True':\n",
        "        args.skip = True\n",
        "    else:\n",
        "        args.skip = False\n",
        "    print(args)\n",
        "\n",
        "    seed_number = int(dir.split('_')[8])\n",
        "    print(\"Current seed number:\", seed_number)\n",
        "\n",
        "    all_train_loader, all_test_loader = get_dataloaders(args.dataset, args.batch_size, args.model)\n",
        "\n",
        "    cw0 = os.path.join( pull_dir, 'modelparams.pt' )\n",
        "    if args.model == 'resnet':\n",
        "      worker = resnet18_narrow(args)\n",
        "      worker.load_state_dict(torch.load(cw0, map_location='cpu'), strict=args.skip)\n",
        "    else:\n",
        "      worker = FlexibleMLP(args.hidden_nodes, args.width)\n",
        "      worker.load_state_dict(torch.load(cw0, map_location='cpu'))\n",
        "\n",
        "    # comprehensive analysis of the average of the workers\n",
        "    worker = worker.to(device)\n",
        "    avg_train_loss, avg_train_error, avg_test_loss, avg_test_error = get_model_eval_comprehensive(worker, all_train_loader, all_test_loader, device)\n",
        "\n",
        "    print(avg_train_error)\n",
        "\n",
        "    logging.info('Experiment index %d:'%ind)\n",
        "    logging.info('Train error %f:'%avg_train_error)\n",
        "    logging.info('Test error %f:'%avg_test_error)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    shannon_ent = calc_shannon_entropy(worker, all_train_loader, device) # Shannon Entropy calculation\n",
        "\n",
        "    v_csv = [ shannon_ent, avg_train_loss, avg_train_error, avg_test_loss, avg_test_error]\n",
        "    valley_csv.append(v_csv)\n",
        "    logging.info('Flatness measures are calculated with seed %d'%seed_number)\n",
        "    logging.info('Saving the flatness measures to a csv.')\n",
        "\n",
        "    print(\"Saving the experiment values to a csv.\")\n",
        "    pd_valley = pd.DataFrame(valley_csv, columns = [ \"shannon_ent\", \"train_loss\", \"train_error\", \"test_loss\", \"test_error\"])\n",
        "    pd_valley.to_csv(\"%s_analysis.csv\"%(model_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My__hMmADuT0",
        "outputId": "483ad6fc-c928-4deb-dbfe-579acc900336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '0.9', '8', '0.0001', '0.1', '128', 'True', '0', '232', 'mlp', '256', 'CIFAR10', '']\n",
            "Namespace(batch_size=128, dataset='CIFAR10', f='/root/.local/share/jupyter/runtime/kernel-b5d3155e-f25c-4d4f-b42a-c1bc3d76833d.json', gpu=0, hidden_nodes=256, lr=0.1, model='mlp', momentum=0.9, skip=True, wd=0.0001, width=8)\n",
            "Current seed number: 232\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "80.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:183: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "1it [00:31, 31.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the experiment values to a csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -9 python"
      ],
      "metadata": {
        "id": "HhmsYQyPallM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(5)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiL-tPRTWK1T",
        "outputId": "0048c4b8-27a1-4b37-8fd2-3a52f32c4681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.7559,  0.0351, -0.5321, -0.4984,  0.2499])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = torch.nn.Softmax()\n",
        "input = a\n",
        "output = m(input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94McFV0jWjfd",
        "outputId": "d520bf10-0757-44b6-c31c-150c9f5a08f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 1e-10"
      ],
      "metadata": {
        "id": "0GNcWdHivO1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI7cPxZVvPxT",
        "outputId": "13a68f48-86c1-4852-9327-4b28cf7bc634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1e-10"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tf8b2iv6vS2h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}